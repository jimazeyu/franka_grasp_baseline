{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Pose Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved intrinsic parameters for camera 1\n",
    "intrinsic_matrix1 = np.array([[ 1361.57,    0., 956.298],\n",
    "                           [    0., 1361.57, 546.581],\n",
    "                           [    0.,    0.,         1.]])\n",
    "dist_coeffs1 = np.array([[0. , 0. , 0. , 0. , 0.]])\n",
    "\n",
    "# Load your saved intrinsic parameters for camera 2\n",
    "intrinsic_matrix2 = np.array([[ 1361.57,    0., 956.298],\n",
    "                           [    0., 1361.57, 546.581],\n",
    "                           [    0.,    0.,         1.]])\n",
    "dist_coeffs2 = np.array([[0. , 0. , 0. , 0. , 0.]])\n",
    "\n",
    "# Define Charuco board parameters\n",
    "charuco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "board = cv2.aruco.CharucoBoard((5, 5), 0.08, 0.06, charuco_dict)\n",
    "\n",
    "# Function to process an image and estimate pose\n",
    "def estimate_pose(image_path, camera_matrix, dist_coeffs):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    corners, ids, _ = cv2.aruco.detectMarkers(gray, charuco_dict)\n",
    "    \n",
    "    if len(corners) > 0:\n",
    "        ret, charuco_corners, charuco_ids = cv2.aruco.interpolateCornersCharuco(corners, ids, gray, board)\n",
    "        if charuco_ids is not None and len(charuco_corners) > 3:\n",
    "            valid, rvec, tvec = cv2.aruco.estimatePoseCharucoBoard(charuco_corners, charuco_ids, board, camera_matrix, dist_coeffs, None, None)\n",
    "            if valid:\n",
    "                return rvec, tvec\n",
    "    return None, None\n",
    "\n",
    "# Calculate relative pose between two cameras\n",
    "def calculate_relative_pose(rvec1, tvec1, rvec2, tvec2):\n",
    "    # Convert rotation vectors to rotation matrices\n",
    "    R_cam1_to_world, _ = cv2.Rodrigues(rvec1)\n",
    "    R_cam2_to_world, _ = cv2.Rodrigues(rvec2)\n",
    "    \n",
    "    # Calculate rotation matrix from camera 1 to camera 2\n",
    "    R_cam1_to_cam2 = np.dot(R_cam2_to_world, R_cam1_to_world.T)\n",
    "    \n",
    "    # Calculate translation vector from camera 1 to camera 2\n",
    "    t_cam1_to_cam2 = tvec2 - np.dot(R_cam1_to_cam2, tvec1)\n",
    "    \n",
    "    return R_cam1_to_cam2, t_cam1_to_cam2\n",
    "\n",
    "# Folder paths for images from camera 1 and camera 2\n",
    "img_folder1 = './cameras/camera1_rgb/'\n",
    "img_folder2 = './cameras/camera2_rgb/'\n",
    "\n",
    "# Get list of image paths for camera 1 and camera 2\n",
    "image_paths1 = [os.path.join(img_folder1, f) for f in os.listdir(img_folder1) if f.endswith('.png')]\n",
    "image_paths2 = [os.path.join(img_folder2, f) for f in os.listdir(img_folder2) if f.endswith('.png')]\n",
    "\n",
    "# List to store relative poses\n",
    "relative_poses = []\n",
    "\n",
    "# Loop through all image pairs and calculate relative pose\n",
    "for image_path1, image_path2 in zip(image_paths1, image_paths2):\n",
    "    # Estimate pose for camera 1\n",
    "    rvec1, tvec1 = estimate_pose(image_path1, intrinsic_matrix1, dist_coeffs1)\n",
    "    \n",
    "    # Estimate pose for camera 2\n",
    "    rvec2, tvec2 = estimate_pose(image_path2, intrinsic_matrix2, dist_coeffs2)\n",
    "    \n",
    "    if rvec1 is not None and rvec2 is not None:\n",
    "        # Calculate relative pose\n",
    "        relative_pose = calculate_relative_pose(rvec1, tvec1, rvec2, tvec2)\n",
    "        relative_poses.append(relative_pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative rotation:\n",
      "[[-0.988019   -0.04995828  0.14602271]\n",
      " [ 0.12685739 -0.8017488   0.58404286]\n",
      " [ 0.08789575  0.5955695   0.79848063]]\n",
      "Average relative translation:\n",
      "[[-0.01313535]\n",
      " [-0.62815895]\n",
      " [ 0.74457722]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average relative pose\n",
    "R_avg = np.zeros((3, 3))\n",
    "t_avg = np.zeros((3, 1))\n",
    "for R, t in relative_poses:\n",
    "    R_avg += R\n",
    "    t_avg += t\n",
    "\n",
    "# make sure R_avg is a proper rotation matrix\n",
    "R_avg /= len(relative_poses)\n",
    "U, S, Vt = np.linalg.svd(R_avg)\n",
    "R_avg = np.dot(U, Vt)\n",
    "\n",
    "t_avg /= len(relative_poses)\n",
    "\n",
    "# Print average relative pose\n",
    "print('Average relative rotation:')\n",
    "print(R_avg)\n",
    "print('Average relative translation:')\n",
    "print(t_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.02707475560353572\n",
      "Error: 0.07303674634444195\n",
      "Error: 0.10426462031860617\n",
      "Error: 0.028504304866996864\n",
      "Error: 0.07898586339031305\n",
      "Error: 0.042659631820817445\n",
      "Error: 0.06647619749288378\n",
      "Error: 0.058643562887256806\n",
      "Error: 0.060856547953273214\n",
      "Error: 0.08861870515310508\n",
      "Mean error: 0.06291209358312301\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean error\n",
    "tot_error = 0\n",
    "for R, t in relative_poses:\n",
    "    error = np.linalg.norm(R - R_avg) + np.linalg.norm(t - t_avg)\n",
    "    print('Error:', error)\n",
    "    tot_error += error\n",
    "mean_error = tot_error / len(relative_poses)\n",
    "print('Mean error:', mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Define the original rotation matrix and translation vector\n",
    "R_original = np.eye(3)  # Identity matrix for no rotation\n",
    "t_original = np.zeros((3, 1))  # Zero translation\n",
    "\n",
    "# Define the relative rotation matrix and translation vector\n",
    "R_relative = R_avg.copy()\n",
    "t_relative = t_avg.copy()\n",
    "\n",
    "# Create a 4x4 transformation matrix for the original pose\n",
    "T_original = np.eye(4)\n",
    "T_original[:3, :3] = R_original\n",
    "T_original[:3, 3] = t_original.flatten()\n",
    "\n",
    "# Create a 4x4 transformation matrix for the relative pose\n",
    "T_relative = np.eye(4)\n",
    "T_relative[:3, :3] = R_relative\n",
    "T_relative[:3, 3] = t_relative.flatten()\n",
    "\n",
    "# Combine the transformations to get the transformed pose\n",
    "T_transformed = np.dot(T_original, T_relative)\n",
    "\n",
    "# Create coordinate frames for the original and transformed poses\n",
    "mesh_frame_original = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6)\n",
    "mesh_frame_transformed = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6)\n",
    "\n",
    "# Transform the coordinate frames\n",
    "mesh_frame_original.transform(T_original)\n",
    "mesh_frame_transformed.transform(T_transformed)\n",
    "\n",
    "# Visualize both poses\n",
    "o3d.visualization.draw_geometries([mesh_frame_original, mesh_frame_transformed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple accumulation of the pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have these variables already defined\n",
    "depth_folder1 = './cameras/camera1_depth/'\n",
    "depth_folder2 = './cameras/camera2_depth/'\n",
    "\n",
    "depth_paths1 = [os.path.join(depth_folder1, f) for f in os.listdir(depth_folder1) if f.endswith('.png')]\n",
    "depth_paths2 = [os.path.join(depth_folder2, f) for f in os.listdir(depth_folder2) if f.endswith('.png')]\n",
    "\n",
    "\n",
    "camera1_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera1_intrinsic.set_intrinsics(1920, 1080, 1361.57, 1361.57, 956.298, 546.581)\n",
    "# camera1_intrinsic = o3d.core.Tensor(camera1_intrinsic.intrinsic_matrix, device=o3d.core.Device('cpu:0'))\n",
    "\n",
    "camera2_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera2_intrinsic.set_intrinsics(1920, 1080, 1361.57, 1361.57, 956.298, 546.581)\n",
    "# camera2_intrinsic = o3d.core.Tensor(camera2_intrinsic.intrinsic_matrix, device=o3d.core.Device('cpu:0'))\n",
    "\n",
    "\n",
    "extrinsics2 = np.eye(4)\n",
    "extrinsics1 = np.eye(4)\n",
    "extrinsics1[:3, :3] = R_avg\n",
    "extrinsics1[:3, 3] = t_avg.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 4000\n",
    "# Create RGBD image\n",
    "rgbd_image1 = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    o3d.geometry.Image(cv2.imread(image_paths1[0])),\n",
    "    o3d.geometry.Image(cv2.imread(depth_paths1[0])),\n",
    "    depth_scale=1000 /ratio,\n",
    "    # depth_trunc=3.0\n",
    ")\n",
    "\n",
    "rgbd_image2 = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    o3d.geometry.Image(cv2.imread(image_paths2[0])),\n",
    "    o3d.geometry.Image(cv2.imread(depth_paths2[0])),\n",
    "    depth_scale=1800 /ratio,\n",
    "    # depth_trunc=3.0\n",
    ")\n",
    "\n",
    "pcd1 = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "    rgbd_image1,\n",
    "    camera1_intrinsic\n",
    ")\n",
    "\n",
    "pcd2 = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "    rgbd_image2,\n",
    "    camera2_intrinsic\n",
    ")\n",
    "\n",
    "# transform point cloud\n",
    "pcd1.transform(extrinsics1)\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([pcd1,pcd2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSDF Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import open3d.core as o3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_two_images(depth_image1, depth_image2, depth_intrinsic_matrix1, depth_intrinsic_matrix2, extrinsics1, extrinsics2, config):\n",
    "    device = o3d.core.Device(config.device)\n",
    "\n",
    "    if config.integrate_color:\n",
    "        vbg = o3d.t.geometry.VoxelBlockGrid(\n",
    "            attr_names=('tsdf', 'weight', 'color'),\n",
    "            attr_dtypes=(o3c.float32, o3c.float32, o3c.float32),\n",
    "            attr_channels=((1), (1), (3)),\n",
    "            voxel_size=3.0 / 512,\n",
    "            block_resolution=16,\n",
    "            block_count=50000,\n",
    "            device=device)\n",
    "    else:\n",
    "        vbg = o3d.t.geometry.VoxelBlockGrid(attr_names=('tsdf', 'weight'),\n",
    "                                            attr_dtypes=(o3c.float32,\n",
    "                                                         o3c.float32),\n",
    "                                            attr_channels=((1), (1)),\n",
    "                                            voxel_size=3.0 / 512,\n",
    "                                            block_resolution=16,\n",
    "                                            block_count=50000,\n",
    "                                            device=device)\n",
    "\n",
    "\n",
    "    # Integrate depth information from the first image\n",
    "    depth1 = o3d.t.io.read_image(depth_image1).to(device)\n",
    "    depth1 = depth1.to(o3c.float32)\n",
    "\n",
    "    frustum_block_coords1 = vbg.compute_unique_block_coordinates(depth1, depth_intrinsic_matrix2, extrinsics1,\n",
    "                                                                 config.depth_scale, config.depth_max)\n",
    "    vbg.integrate(frustum_block_coords1, depth1, depth_intrinsic_matrix1, extrinsics1, config.depth_scale, config.depth_max)\n",
    "\n",
    "    # # Integrate depth information from the second image\n",
    "    # depth2 = o3d.t.io.read_image(depth_image2).to(device)\n",
    "    # depth2 = depth2.to(o3c.float32)\n",
    "\n",
    "    # frustum_block_coords2 = vbg.compute_unique_block_coordinates(depth2, depth_intrinsic_matrix2, extrinsics2,\n",
    "    #                                                              config.depth_scale, config.depth_max)\n",
    "    # vbg.integrate(frustum_block_coords2, depth2, depth_intrinsic_matrix2, extrinsics2, config.depth_scale, config.depth_max)\n",
    "\n",
    "    return vbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera1_intrinsic = o3d.core.Tensor(camera1_intrinsic.intrinsic_matrix, device=o3d.core.Device('cpu:0'))\n",
    "camera2_intrinsic = o3d.core.Tensor(camera2_intrinsic.intrinsic_matrix, device=o3d.core.Device('cpu:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\u001b[1;31m[Open3D Error] (void open3d::t::geometry::PointCloud::SetPointColors(const open3d::core::Tensor&)) /root/Open3D/cpp/open3d/t/geometry/PointCloud.h:173: Tensor has shape {0}, but is expected to have compatible with {None, 3}.\n\u001b[0;m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Integrate depth information from the two images\u001b[39;00m\n\u001b[1;32m     10\u001b[0m vbg \u001b[38;5;241m=\u001b[39m integrate_two_images(depth_paths1[\u001b[38;5;241m0\u001b[39m], depth_paths2[\u001b[38;5;241m0\u001b[39m], camera1_intrinsic, camera2_intrinsic, extrinsics1, extrinsics2, config)\n\u001b[0;32m---> 12\u001b[0m pcd \u001b[38;5;241m=\u001b[39m \u001b[43mvbg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_point_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m o3d\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mdraw([pcd])\n\u001b[1;32m     15\u001b[0m mesh \u001b[38;5;241m=\u001b[39m vbg\u001b[38;5;241m.\u001b[39mextract_triangle_mesh()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \u001b[1;31m[Open3D Error] (void open3d::t::geometry::PointCloud::SetPointColors(const open3d::core::Tensor&)) /root/Open3D/cpp/open3d/t/geometry/PointCloud.h:173: Tensor has shape {0}, but is expected to have compatible with {None, 3}.\n\u001b[0;m"
     ]
    }
   ],
   "source": [
    "# config(integrate_color, device, depth_scale, depth_max)\n",
    "from config import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.integrate_color = False\n",
    "config.device = 'cpu:0'\n",
    "config.depth_scale = 1800 / 4000\n",
    "config.depth_max = 3.0\n",
    "\n",
    "# Integrate depth information from the two images\n",
    "vbg = integrate_two_images(depth_paths1[0], depth_paths2[0], camera1_intrinsic, camera2_intrinsic, extrinsics1, extrinsics2, config)\n",
    "\n",
    "pcd = vbg.extract_point_cloud()\n",
    "o3d.visualization.draw([pcd])\n",
    "\n",
    "mesh = vbg.extract_triangle_mesh()\n",
    "o3d.visualization.draw([mesh.to_legacy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
