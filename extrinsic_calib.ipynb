{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Pose Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the images\n",
    "base_dir = './images'\n",
    "rgb_folder1 = f'{base_dir}/calib/camera1_rgb'\n",
    "rgb_folder2 = f'{base_dir}/calib/camera2_rgb'\n",
    "depth_folder1 = f'{base_dir}/calib/camera1_depth'\n",
    "depth_folder2 = f'{base_dir}/calib/camera2_depth'\n",
    "\n",
    "# Get list of image paths for camera 1 and camera 2\n",
    "rgb_paths1 = [os.path.join(rgb_folder1, f) for f in os.listdir(rgb_folder1) if f.endswith('.png')]\n",
    "rgb_paths2 = [os.path.join(rgb_folder2, f) for f in os.listdir(rgb_folder1) if f.endswith('.png')]\n",
    "depth_paths1 = [os.path.join(depth_folder1, f) for f in os.listdir(depth_folder1) if f.endswith('.npy')]\n",
    "depth_paths2 = [os.path.join(depth_folder2, f) for f in os.listdir(depth_folder2) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic matrix 1: [[1.36377332e+03 0.00000000e+00 9.56298462e+02]\n",
      " [0.00000000e+00 1.36157141e+03 5.46581238e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Intrinsic matrix 2: [[1.36576904e+03 0.00000000e+00 9.50094910e+02]\n",
      " [0.00000000e+00 1.36530176e+03 5.55031494e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Distortion coefficients 1: [0 0 0 0 0]\n",
      "Distortion coefficients 2: [0 0 0 0 0]\n",
      "Depth scale 1: 0.0010000000474974513\n",
      "Depth scale 2: 0.0010000000474974513\n"
     ]
    }
   ],
   "source": [
    "# Load the intrinsic and extrinsic parameters\n",
    "camera1_params = np.load(f'{base_dir}/calib/camera1_params.npz')\n",
    "camera2_params = np.load(f'{base_dir}/calib/camera2_params.npz')\n",
    "intrinsic_matrix1 = camera1_params['color_intrinsics']\n",
    "intrinsic_matrix2 = camera2_params['color_intrinsics']\n",
    "dist_coeffs1 = camera1_params['color_distortion']\n",
    "dist_coeffs2 = camera2_params['color_distortion']\n",
    "depth_scale1 = camera1_params['depth_scale']\n",
    "depth_scale2 = camera2_params['depth_scale']\n",
    "\n",
    "print('Intrinsic matrix 1:', intrinsic_matrix1)\n",
    "print('Intrinsic matrix 2:', intrinsic_matrix2)\n",
    "print('Distortion coefficients 1:', dist_coeffs1)\n",
    "print('Distortion coefficients 2:', dist_coeffs2)\n",
    "print('Depth scale 1:', depth_scale1)\n",
    "print('Depth scale 2:', depth_scale2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Charuco board parameters\n",
    "charuco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "board = cv2.aruco.CharucoBoard((5, 5), 0.08, 0.06, charuco_dict)\n",
    "\n",
    "# Function to process an image and estimate pose\n",
    "def estimate_pose(image_path, camera_matrix, dist_coeffs):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    corners, ids, _ = cv2.aruco.detectMarkers(gray, charuco_dict)\n",
    "    \n",
    "    if len(corners) > 0:\n",
    "        ret, charuco_corners, charuco_ids = cv2.aruco.interpolateCornersCharuco(corners, ids, gray, board)\n",
    "        if charuco_ids is not None and len(charuco_corners) > 3:\n",
    "            valid, rvec, tvec = cv2.aruco.estimatePoseCharucoBoard(charuco_corners, charuco_ids, board, camera_matrix, dist_coeffs, None, None)\n",
    "            if valid:\n",
    "                return rvec, tvec\n",
    "    return None, None\n",
    "\n",
    "# Calculate relative pose between two cameras\n",
    "def calculate_relative_pose(rvec1, tvec1, rvec2, tvec2):\n",
    "    # Convert rotation vectors to rotation matrices\n",
    "    R_cam1_to_world, _ = cv2.Rodrigues(rvec1)\n",
    "    R_cam2_to_world, _ = cv2.Rodrigues(rvec2)\n",
    "    \n",
    "    # Calculate rotation matrix from camera 1 to camera 2\n",
    "    R_cam1_to_cam2 = np.dot(R_cam2_to_world, R_cam1_to_world.T)\n",
    "    \n",
    "    # Calculate translation vector from camera 1 to camera 2\n",
    "    t_cam1_to_cam2 = tvec2 - np.dot(R_cam1_to_cam2, tvec1)\n",
    "    \n",
    "    return R_cam1_to_cam2, t_cam1_to_cam2\n",
    "\n",
    "# List to store relative poses\n",
    "relative_poses = []\n",
    "\n",
    "# Loop through all image pairs and calculate relative pose\n",
    "for image_path1, image_path2 in zip(rgb_paths1, rgb_paths2):\n",
    "    # Estimate pose for camera 1\n",
    "    rvec1, tvec1 = estimate_pose(image_path1, intrinsic_matrix1, dist_coeffs1)\n",
    "    \n",
    "    # Estimate pose for camera 2\n",
    "    rvec2, tvec2 = estimate_pose(image_path2, intrinsic_matrix2, dist_coeffs2)\n",
    "    \n",
    "    if rvec1 is not None and rvec2 is not None:\n",
    "        # Calculate relative pose\n",
    "        relative_pose = calculate_relative_pose(rvec1, tvec1, rvec2, tvec2)\n",
    "        relative_poses.append(relative_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative rotation:\n",
      "[[-0.98752715 -0.04611448  0.1505443 ]\n",
      " [ 0.12623187 -0.80335989  0.58196083]\n",
      " [ 0.09410443  0.5937056   0.79916082]]\n",
      "Average relative translation:\n",
      "[[-0.00960099]\n",
      " [-0.63541039]\n",
      " [ 0.74765755]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average relative pose\n",
    "R_avg = np.zeros((3, 3))\n",
    "t_avg = np.zeros((3, 1))\n",
    "for R, t in relative_poses:\n",
    "    R_avg += R\n",
    "    t_avg += t\n",
    "\n",
    "# make sure R_avg is a proper rotation matrix\n",
    "R_avg /= len(relative_poses)\n",
    "U, S, Vt = np.linalg.svd(R_avg)\n",
    "R_avg = np.dot(U, Vt)\n",
    "\n",
    "t_avg /= len(relative_poses)\n",
    "\n",
    "# Print average relative pose\n",
    "print('Average relative rotation:')\n",
    "print(R_avg)\n",
    "print('Average relative translation:')\n",
    "print(t_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.08818418242638391\n",
      "Error: 0.010917324224961451\n",
      "Error: 0.07693433539884519\n",
      "Error: 0.04076680274855825\n",
      "Error: 0.0851724390459744\n",
      "Error: 0.09222404627766553\n",
      "Error: 0.0767770782053251\n",
      "Mean error: 0.06728231547538768\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean error\n",
    "tot_error = 0\n",
    "for R, t in relative_poses:\n",
    "    error = np.linalg.norm(R - R_avg) + np.linalg.norm(t - t_avg)\n",
    "    print('Error:', error)\n",
    "    tot_error += error\n",
    "mean_error = tot_error / len(relative_poses)\n",
    "print('Mean error:', mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Define the original rotation matrix and translation vector\n",
    "R_original = np.eye(3)  # Identity matrix? for no rotation\n",
    "t_original = np.zeros((3, 1))  # Zero translation\n",
    "\n",
    "# Define the relative rotation matrix and translation vector\n",
    "R_relative = R_avg.copy()\n",
    "t_relative = t_avg.copy()\n",
    "\n",
    "# Create a 4x4 transformation matrix for the original pose\n",
    "T_original = np.eye(4)\n",
    "T_original[:3, :3] = R_original\n",
    "T_original[:3, 3] = t_original.flatten()\n",
    "\n",
    "# Create a 4x4 transformation matrix for the relative pose\n",
    "T_relative = np.eye(4)\n",
    "T_relative[:3, :3] = R_relative\n",
    "T_relative[:3, 3] = t_relative.flatten()\n",
    "\n",
    "# Combine the transformations to get the transformed pose\n",
    "T_transformed = np.dot(T_original, T_relative)\n",
    "\n",
    "# Create coordinate frames for the original and transformed poses\n",
    "mesh_frame_original = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6)\n",
    "mesh_frame_transformed = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6)\n",
    "\n",
    "# Transform the coordinate frames\n",
    "mesh_frame_original.transform(T_original)\n",
    "mesh_frame_transformed.transform(T_transformed)\n",
    "\n",
    "# Visualize both poses\n",
    "o3d.visualization.draw_geometries([mesh_frame_original, mesh_frame_transformed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple accumulation of the pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the images\n",
    "base_dir = './images'\n",
    "rgb_folder1 = f'{base_dir}/test/camera1_rgb'\n",
    "rgb_folder2 = f'{base_dir}/test/camera2_rgb'\n",
    "depth_folder1 = f'{base_dir}/test/camera1_depth'\n",
    "depth_folder2 = f'{base_dir}/test/camera2_depth'\n",
    "\n",
    "# Get list of image paths for camera 1 and camera 2\n",
    "rgb_paths1 = [os.path.join(rgb_folder1, f) for f in os.listdir(rgb_folder1) if f.endswith('.png')]\n",
    "rgb_paths2 = [os.path.join(rgb_folder2, f) for f in os.listdir(rgb_folder1) if f.endswith('.png')]\n",
    "depth_paths1 = [os.path.join(depth_folder1, f) for f in os.listdir(depth_folder1) if f.endswith('.npy')]\n",
    "depth_paths2 = [os.path.join(depth_folder2, f) for f in os.listdir(depth_folder2) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera1_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera1_intrinsic.set_intrinsics(intrinsic_matrix1.shape[1], intrinsic_matrix1.shape[0], intrinsic_matrix1[0, 0], intrinsic_matrix1[1, 1], intrinsic_matrix1[0, 2], intrinsic_matrix1[1, 2])\n",
    "\n",
    "camera2_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera2_intrinsic.set_intrinsics(intrinsic_matrix2.shape[1], intrinsic_matrix2.shape[0], intrinsic_matrix2[0, 0], intrinsic_matrix2[1, 1], intrinsic_matrix2[0, 2], intrinsic_matrix2[1, 2])\n",
    "\n",
    "extrinsics2 = np.eye(4)\n",
    "extrinsics1 = np.eye(4)\n",
    "extrinsics1[:3, :3] = R_avg\n",
    "extrinsics1[:3, 3] = t_avg.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 1729770 points."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_img1 = cv2.imread(rgb_paths1[0])\n",
    "depth_img1 = np.load(depth_paths1[0])\n",
    "rgb_img2 = cv2.imread(rgb_paths2[0])\n",
    "depth_img2 = np.load(depth_paths2[0])\n",
    "# Create RGBD image\n",
    "rgbd_image1 = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    o3d.geometry.Image(rgb_img1),\n",
    "    o3d.geometry.Image(depth_img1),\n",
    "    depth_scale=1.0/depth_scale1,\n",
    "    # depth_trunc=3.0\n",
    ")\n",
    "\n",
    "rgbd_image2 = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    o3d.geometry.Image(rgb_img2),\n",
    "    o3d.geometry.Image(depth_img2),\n",
    "    depth_scale=1.0/depth_scale2,\n",
    "    # depth_trunc=3.0\n",
    ")\n",
    "\n",
    "pcd1 = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "    rgbd_image1,\n",
    "    camera1_intrinsic\n",
    ")\n",
    "\n",
    "pcd2 = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "    rgbd_image2,\n",
    "    camera2_intrinsic\n",
    ")\n",
    "\n",
    "# transform point cloud\n",
    "pcd1.transform(extrinsics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export the pointcloud as .ply\n",
    "o3d.io.write_point_cloud(\"output_point_cloud.ply\", pcd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd1 to blue\n",
    "pcd1.paint_uniform_color([0, 0, 1])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([pcd1,pcd2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
